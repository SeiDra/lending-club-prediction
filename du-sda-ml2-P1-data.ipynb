{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"projet-7\"></a>\n",
    "# PROJET 7 : Loan Default Prediction #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"contenu\"></a>\n",
    "## Contenu Partie N°1 ##\n",
    "- [Import des données](#import-des-donnees)\n",
    "- [CSV en Parquet](#csv-en-parquet)\n",
    "- [Exploration et data cleaning](#exploration-et-data-cleaning)\n",
    "- [EDA Automation](#EDA-Automation)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "  - [Removing exclusions](#Removing-exclusions)\n",
    "  - [Missing Value Imputation](#Missing-Value-Imputation)\n",
    "  - [Removing Outlier](#Removing-Outlier)\n",
    "- [Correlation Analysis](#Correlation-Analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Objectifs\"></a>\n",
    "## Objectifs ##\n",
    "\n",
    "\n",
    "_**Contexte :**_\n",
    "- Les institutions financières doivent évaluer le risque de défaut de paiement des prêts.\n",
    "- Des prédictions précises peuvent aider à prendre des décisions de prêt.\n",
    "- Le Machine learning peut analyser l'historique des données afin de prédire les défauts.\n",
    "\n",
    "_**Objectifs:**_\n",
    "- Construire un modèle prédictif de défaut de paiement basé sur le profil des emprunteurs.\n",
    "- Identifier les variables clés qui influent le plus sur le risque de défaut.\n",
    "- Fournir des recommendations pour atténuer les risques.\n",
    "\n",
    "_**Origine des données**_\n",
    "\n",
    "- \"All Lending Club loan data\" (2007 through current Lending Club accepted and rejected loan data)\n",
    "- URL : https://www.kaggle.com/datasets/wordsforthewise/lending-club/data\n",
    "\n",
    "\n",
    "_**Défis:**_\n",
    "- Traitement des classes déséquilibrées: gérer les jeux de données où les cas de défaut son minoritaires.\n",
    "- Feature engineering: extraire les indicateurs les plus pertinents du profil des candidats.\n",
    "- Interprétabilité du modèle: transformer les résultats techniques enleviers d'action concrets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-12-13T03:48:41.56034Z",
     "iopub.status.busy": "2023-12-13T03:48:41.559958Z",
     "iopub.status.idle": "2023-12-13T03:49:02.60048Z",
     "shell.execute_reply": "2023-12-13T03:49:02.599016Z",
     "shell.execute_reply.started": "2023-12-13T03:48:41.560307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# start_time = datetime.now()\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from matplotlib import rcParams\n",
    "\n",
    "# %matplotlib inline\n",
    "# # figure size in inches\n",
    "# rcParams['figure.figsize'] = 8,6\n",
    "\n",
    "\n",
    "# # Plotly visualizations\n",
    "# from plotly import tools\n",
    "# # import chart_studio.plotly as py\n",
    "# import plotly.figure_factory as ff\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# import plotly.offline as pyo\n",
    "# import plotly.express as px\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# # For oversampling Library (Dealing with Imbalanced Datasets)\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from collections import Counter\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# import math\n",
    "# import itertools as it\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Classifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Other machine learning tools\n",
    "\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# import scipy.stats as sps\n",
    "\n",
    "# print('LOADING DURATION: ', datetime.now() - start_time)\n",
    "\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"import-des-donnees\"></a>\n",
    "## [Import des données](#contenu) ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA/accepted_2007_to_2018Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction du format de 'id'\n",
    "\n",
    "df['id'] = df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la colonne issue_d en format datetime\n",
    "\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"csv-en-parquet\"></a>\n",
    "## [CSV en Parquet](#contenu) ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder en parquet\n",
    "\n",
    "df.to_parquet(\"DATA/accepted_2007_to_2018Q4.parquet\")\n",
    "\n",
    "print(\"Conversion terminée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer la mémoire vive (RAM)\n",
    "\n",
    "# 1. Supprimer le gros DataFrame chargé depuis le CSV\n",
    "if 'df' in locals():\n",
    "    del df\n",
    "\n",
    "# 2. Forcer la libération de la mémoire\n",
    "gc.collect()\n",
    "\n",
    "# 3. Charger le nouveau fichier Parquet (beaucoup plus léger)\n",
    "df = pd.read_parquet(\"DATA/accepted_2007_to_2018Q4.parquet\")\n",
    "\n",
    "print(\"Données chargées depuis le format Parquet !\")\n",
    "print(f\"Dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"etude-loan-status-et-issue-d\"></a>\n",
    "## [Etude des colonnes \"loan status\" et \"issue_d\"](#contenu) ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre pour ne garder que les lignes \"Current\"\n",
    "current_loans = df[df['loan_status'] == 'Current']\n",
    "\n",
    "# Calcule des occurrences de issue_d sur ce sous-ensemble\n",
    "counts = current_loans['issue_d'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "print(\"Aperçu des occurrences :\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation en deux groupes en utilisant une simple comparaison de chaînes de caractères\n",
    "mask_recent = counts.index >= '2017-01'\n",
    "mask_old = counts.index <= '2016-12'\n",
    "\n",
    "# Calcule de la somme pour chaque groupe\n",
    "count_2017_2018 = counts[mask_recent].sum()\n",
    "count_2016_and_before = counts[mask_old].sum()\n",
    "\n",
    "print(f\"Occurrences de 'Current' (2017-01 à 2018-12) : {count_2017_2018}\")\n",
    "print(f\"Occurrences de 'Current' (2016-12 et inférieur) : {count_2016_and_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait des \"current\" trop récent pour tirer des conclusions \n",
    "lignes_a_supprimer = (df['loan_status'] == 'Current') & (df['issue_d'] >= '2017-01-01')\n",
    "\n",
    "# On met à jour le dataframe\n",
    "df = df[~lignes_a_supprimer].copy()\n",
    "\n",
    "# Vérification\n",
    "print(f\"Nombre de lignes restantes dans le dataframe après nettoyage : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un échantillon\n",
    "\n",
    "# Sélection et concaténation des 100000 premières et 100000 dernières lignes\n",
    "df_echantillon = pd.concat([df.head(100000), df.tail(100000)])\n",
    "\n",
    "# Sauvegarde de cet échantillon dans un nouveau fichier CSV\n",
    "# L'argument index=False évite de sauvegarder les anciens numéros de ligne comme une nouvelle colonne\n",
    "df_echantillon.to_csv(\"DATA/accepted_2007_to_2018Q4_sample_200k.csv\", index=False)\n",
    "\n",
    "# Vérification des dimensions\n",
    "print(f\"L'échantillon a été créé avec succès !\")\n",
    "print(f\"Dimensions du nouvel échantillon : {df_echantillon.shape}\")\n",
    "\n",
    "df = df_echantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exploration-et-data-cleaning\"></a>\n",
    "## [Exploration et data cleaning](#contenu) ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des 5 premières lignes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des 5 dernières lignes\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Retrait des lignes de totalisation\"></a>\n",
    "### Retrait des lignes de totalisation ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cherche les lignes dont l'id contient \"Total\" (insensible à la casse)\n",
    "lignes_total = df[df['id'].str.contains(\"Total\", case=False, na=False)]\n",
    "\n",
    "print(f\"Nombre de lignes de totaux explicites détectées : {len(lignes_total)}\")\n",
    "print(\"\\nContenu de la colonne 'id' pour ces lignes :\")\n",
    "print(lignes_total['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne garde que les lignes qui ne contiennent PAS \"Total\" dans l'id\n",
    "df = df[~df['id'].str.contains(\"Total\", case=False, na=False)]\n",
    "\n",
    "print(f\"Lignes de totaux purgées. Nouvelle taille : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compte des lignes où loan_amnt est manquant\n",
    "nb_null_loan = df['loan_amnt'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de lignes parasites (loan_amnt nul) : {nb_null_loan}\")\n",
    "\n",
    "# Visualiser ces lignes pour confirmer qu'il s'agit bien de totaux\n",
    "print(\"\\nAperçu des colonnes 'id' de ces lignes :\")\n",
    "print(df[df['loan_amnt'].isnull()]['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes où loan_amnt est NaN\n",
    "df = df.dropna(subset=['loan_amnt'])\n",
    "\n",
    "# Vérification de la nouvelle taille du dataset\n",
    "print(f\"Nettoyage terminé. Nouvelles dimensions : {df.shape}\")\n",
    "\n",
    "# Vérification visuelle de la fin du fichier\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Vérification des lignes en doublon\"></a>\n",
    "### Vérification des lignes en doublon ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre total de lignes 100% identiques\n",
    "nb_doublons = df.duplicated().sum()\n",
    "\n",
    "print(f\"Nombre de lignes en doublon détectées : {nb_doublons}\")\n",
    "\n",
    "# Aperçu de ces doublons :\n",
    "if nb_doublons > 0:\n",
    "    print(\"\\nAperçu des lignes dupliquées :\")\n",
    "    display(df[df.duplicated()].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Vérification des colonnes entièrement à Null\"></a>\n",
    "### Vérification des colonnes entièrement à Null ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes 100% nulles\n",
    "\n",
    "all_null_cols = df.columns[df.isnull().all()].tolist()\n",
    "\n",
    "if len(all_null_cols) > 0:\n",
    "    print(f\"Il y a {len(all_null_cols)} colonnes entièrement vides :\")\n",
    "    print(all_null_cols)\n",
    "else:\n",
    "    print(\"Aucune colonne n'est entièrement vide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne member_id\n",
    "\n",
    "df = df.drop(columns=['member_id'])\n",
    "print(\"La colonne 'member_id' a été supprimée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Suppression des colonnes Hardship, Settlement et historique\"></a>\n",
    "### Suppression des colonnes \"Hardship\", \"Settlement\" et historique du remboursement ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de valeurs nulles par colonne\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Affichage des 20 colonnes avec le plus de valeurs manquantes\n",
    "print(\"Synthèse des valeurs manquantes (Top 20) :\")\n",
    "print(null_counts.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des colonnes \"Hardship\" (Plans de difficultés financières)**\n",
    "\n",
    "Ces colonnes concernent les emprunteurs qui ont rencontré de graves problèmes financiers (perte d'emploi, maladie) et qui ont négocié un plan de sauvetage temporaire (\"Hardship plan\") avec Lending Club :\n",
    "\n",
    " *   hardship_reason : La cause de la difficulté (chômage, etc.).\n",
    " *   hardship_type : Le type de plan mis en place.\n",
    " *   hardship_status : Le statut du plan (actif, terminé, annulé).\n",
    " *   hardship_amount : Le montant de la mensualité pendant le plan.\n",
    " *   hardship_start_date / hardship_end_date : Dates de début et de fin du plan.\n",
    " *   hardship_length : La durée du plan en mois.\n",
    " *   hardship_dpd : Nombre de jours de retard de paiement (Days Past Due) au moment du plan.\n",
    " *   hardship_loan_status : Le statut du prêt pendant le plan.\n",
    " *   hardship_payoff_balance_amount : Le montant total restant à payer à l'issue du plan.\n",
    " *   hardship_last_payment_amount : Le dernier paiement effectué sous ce régime.\n",
    " *   payment_plan_start_date : Le jour où le plan a officiellement commencé.\n",
    " *   deferral_term : Le nombre de mois pendant lesquels le paiement a été repoussé.\n",
    " *   orig_projected_additional_accrued_interest : Les intérêts supplémentaires qui vont s'accumuler à cause de ce plan.\n",
    "\n",
    "**Objectif de la suppression : éviter le Data Leakage** \n",
    "\n",
    "Ces informations n'existent que parce que l'emprunteur a déjà cessé de payer normalement. \n",
    "Or, le but du Machine Learning est de prédire le défaut au moment où le prêt est accordé, donc moment où toutes ces colonnes sont inexistantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste exhaustive des colonnes \"Hardship\"\n",
    "cols_hardship = [\n",
    "    'orig_projected_additional_accrued_interest',\n",
    "    'hardship_reason', \n",
    "    'hardship_payoff_balance_amount',\n",
    "    'hardship_last_payment_amount', \n",
    "    'payment_plan_start_date',\n",
    "    'hardship_type', \n",
    "    'hardship_status', \n",
    "    'hardship_start_date',\n",
    "    'deferral_term', \n",
    "    'hardship_amount', \n",
    "    'hardship_dpd',\n",
    "    'hardship_loan_status', \n",
    "    'hardship_length', \n",
    "    'hardship_end_date',\n",
    "    'hardship_flag'  \n",
    "]\n",
    "\n",
    "# Suppression des colonnes (errors='ignore' évite un plantage si une colonne est déjà absente)\n",
    "df = df.drop(columns=cols_hardship, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes 'Hardship' ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des colonnes \"Settlement\" (Règlements de dettes)**\n",
    "\n",
    "Ces colonnes s'activent lorsqu'un prêt a définitivement fait défaut (\"Charged Off\") et que le service de recouvrement négocie avec l'emprunteur pour récupérer au moins une partie de l'argent (par exemple, solder la dette pour 40% du montant restant) :\n",
    "\n",
    " *   settlement_status : Le statut de l'accord de règlement (ex: complet, rompu).\n",
    " *   settlement_date : La date à laquelle le recouvrement a accepté le règlement.\n",
    " *   settlement_amount : Le montant final convenu pour clore la dette.\n",
    " *   settlement_percentage : Le pourcentage de la dette initiale que représente le règlement.\n",
    " *   settlement_term : Le nombre de mois accordés pour payer ce règlement.\n",
    " *   debt_settlement_flag_date : La date à laquelle le dossier a été marqué comme \"en cours de règlement\".\n",
    "\n",
    "\n",
    "**Objectif de la suppression : éviter le Data Leakage** \n",
    "\n",
    "Exactement comme pour le groupe \"Hardship\", un modèle qui voit un montant dans settlement_amount saura à 100% que le prêt est mauvais, rendant la prédiction inutile et biaisée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste exhaustive des colonnes \"Settlement\"\n",
    "cols_settlement = [\n",
    "    'settlement_status',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_term',\n",
    "    'settlement_percentage',\n",
    "    'settlement_date',\n",
    "    'settlement_amount',\n",
    "    'debt_settlement_flag'  \n",
    "]\n",
    "\n",
    "# Suppression des colonnes (errors='ignore' évite un plantage si une colonne est déjà absente)\n",
    "df = df.drop(columns=cols_settlement, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes 'Settlement' ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des colonnes liée à l'historique de remboursement du prêt et son état actuel**\n",
    "\n",
    " Ces variables décrivent l'historique de remboursement du prêt et son état actuel :\n",
    "\n",
    " *   out_prncp / out_prncp_inv : Capital restant dû (principal) sur le montant total financé / sur la part financée par les investisseurs.\n",
    " *   total_pymnt / total_pymnt_inv : Total des paiements reçus à ce jour pour le montant total financé / pour la part des investisseurs.\n",
    " *   total_rec_prncp : Capital (principal) remboursé par l'emprunteur à ce jour.\n",
    " *   total_rec_int : Intérêts remboursés par l'emprunteur à ce jour.\n",
    " *   total_rec_late_fee : Frais ou pénalités de retard perçus à ce jour.\n",
    " *   recoveries : Montants récupérés après que le prêt a été déclaré en perte (procédure de recouvrement post \"charge-off\").\n",
    " *   collection_recovery_fee : Frais de gestion facturés par les agences de recouvrement pour récupérer les fonds.\n",
    " *   last_pymnt_amnt : Montant du tout dernier paiement total reçu.\n",
    " *   last_pymnt_d : Date (mois) à laquelle le dernier paiement a été enregistré.\n",
    " *   last_credit_pull_d : Date la plus récente à laquelle le prêteur a consulté le dossier de crédit de l'emprunteur.\n",
    " *   next_pymnt_d : Date du prochain paiement.\n",
    "\n",
    "\n",
    "**Objectif de la suppression : éviter le Data Leakage** \n",
    "\n",
    "Dans un projet de prédiction de défaut de paiement, l'objectif est de déterminer si un emprunteur va faire défaut au moment où il demande son prêt. Or, ces informations (paiements reçus, reliquat du capital, frais de retard) ne sont connues qu'après que le prêt a été accordé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes de \"Data Leakage\" (données futures ou liées au comportement de remboursement)\n",
    "cols_leakage = [\n",
    "    'out_prncp', \n",
    "    'out_prncp_inv', \n",
    "    'total_pymnt', \n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp', \n",
    "    'total_rec_int', \n",
    "    'total_rec_late_fee',\n",
    "    'recoveries', \n",
    "    'collection_recovery_fee', \n",
    "    'last_pymnt_amnt',\n",
    "    'last_pymnt_d', \n",
    "    'last_credit_pull_d',\n",
    "    'next_pymnt_d'\n",
    "]\n",
    "\n",
    "# Suppression des colonnes\n",
    "df = df.drop(columns=cols_leakage, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes de 'Data Leakage' ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression de la colonne \"issue_d\"**\n",
    "\n",
    "Ce n'est techniquement pas du Data Leakage.\n",
    "L'objectif est de prédire si un emprunteur va faire défaut avant de lui accorder le prêt.\n",
    "Or, au moment où le client fait sa demande, la date du jour (qui deviendra le mois et l'année de issue_d) est connue.\n",
    "\n",
    "Mais si l'on donne l'année \"2014\" ou \"2015\" à un algorithme, il risque d'apprendre des règles du type : \"les prêts de 2014 ont eu beaucoup de défauts, donc l'année 2014 est un risque\".\n",
    "\n",
    "Le problème ? Quand vous déploierez ce modèle en production en 2025 ou 2026, l'algorithme ne saura pas comment interpréter ces nouvelles années qu'il n'a jamais vues pendant son entraînement. Il va donc perdre en performance.\n",
    "\n",
    "Conserver issue_d dans les features risque d'apporter plus de problèmes (overfitting temporel) que de solutions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes\n",
    "cols_leakage = ['issue_d']\n",
    "df = df.drop(columns=cols_leakage, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes de 'Data Leakage' ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Suppression des lignes des prêts conjoints\"></a>\n",
    "### Suppression des lignes des \"prêts conjoints\" ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de valeurs nulles par colonne\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Affichage des 20 colonnes avec le plus de valeurs manquantes\n",
    "print(\"Synthèse des valeurs manquantes (Top 20) :\")\n",
    "print(null_counts.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des lignes avec un \"Co-emprunteur\" (sec_app_... et ..._joint)**\n",
    "\n",
    "Des colonnes (comme annual_inc_joint ou sec_app_fico_range_low) correspondent aux informations d'un second demandeur lorsque le prêt est fait à deux (demande conjointe).\n",
    "\n",
    "Or, les prêts conjoints (à deux emprunteurs) ont une dynamique de risque très différente des prêts individuels (les revenus et les dettes sont cumulés). Mélanger les deux dans un même modèle peut embrouiller l'algorithme. \n",
    "\n",
    "Dans la mesure où ces prêts conjoints ne représentent que quelques pourcents du dataset, il semble préférable de les retirer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir combien il y a de prêts de chaque type\n",
    "print(\"Répartition des types de prêts avant filtrage :\")\n",
    "print(df['application_type'].value_counts())\n",
    "\n",
    "# Ne conserver que les prêts individuels\n",
    "df = df[df['application_type'] == 'Individual']\n",
    "\n",
    "# Supprimer la colonne 'application_type' devenue inutile\n",
    "df = df.drop(columns=['application_type'])\n",
    "\n",
    "print(f\"\\nPrêts conjoints supprimés ! Nouvelles dimensions : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des colonnes ne contenant aucune donnée**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes 100% nulles\n",
    "\n",
    "all_null_cols = df.columns[df.isnull().all()].tolist()\n",
    "\n",
    "if len(all_null_cols) > 0:\n",
    "    print(f\"Il y a {len(all_null_cols)} colonnes entièrement vides :\")\n",
    "    print(all_null_cols)\n",
    "else:\n",
    "    print(\"Aucune colonne n'est entièrement vide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les colonnes qui sont 100% vides\n",
    "df = df.dropna(axis=1, how='all')\n",
    "print(f\"Purge terminée ! Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de valeurs nulles par colonne\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Affichage des 20 colonnes avec le plus de valeurs manquantes\n",
    "print(\"Synthèse des valeurs manquantes (Top 20) :\")\n",
    "print(null_counts.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colonnes contenant du texte libre ou des données non structurées**\n",
    "\n",
    " *   emp_title (Titre du poste) : Le nom du poste ou métier renseigné librement par l'emprunteur lors de sa demande. Étant un champ de texte libre, il contient d'innombrables fautes de frappe, abréviations et variantes pour un même métier. En l'éat, il y a trop de valeurs uniques. Sans un traitement lourd, cette variable va faire \"planter\" ou sur-apprendre les modèles.\n",
    "\n",
    " *   desc (Description du prêt) : Un paragraphe rédigé par l'emprunteur pour expliquer pourquoi il a besoin de ce prêt. C'est du texte pur. Bien qu'il puisse contenir du sens, l'extraire demanderait des techniques de traitement qui sortent du cadre d'une modélisation standard.\n",
    "\n",
    " *   title (Titre du prêt) : Un titre court donné par l'emprunteur à son prêt (ex: \"Debt Consolidation\", \"Credit Card Payoff\"). Il fait doublon avec la colonne \"purpose\". Cependant, purpose est une liste déroulante contrôlée (donc propre et standardisée), alors que title est un texte libre soumis aux mêmes problèmes que emp_title.\n",
    "\n",
    " *   url (Lien URL) : L'adresse web de la page de la demande de prêt sur le site de Lending Club. Un lien web ne contient absolument aucune information statistique ou mathématique exploitable pour évaluer la solvabilité d'un individu.\n",
    "\n",
    " *   zip_code (Code postal) : Les trois premiers chiffres du code postal de l'emprunteur (pour des raisons d'anonymat, LC masque les derniers chiffres, ex: \"902xx\"). Même tronqués, les codes postaux génèrent beaucoup trop de catégories différentes. C'est trop lourd pour l'encodage. De plus, l'information géographique pertinente est déjà capturée de manière beaucoup plus propre par la colonne addr_state (l'État américain).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes de texte libre et métadonnées non structurées\n",
    "cols_unstructured = [\n",
    "    'emp_title', \n",
    "    'desc', \n",
    "    'title', \n",
    "    'url', \n",
    "    'zip_code'\n",
    "]\n",
    "\n",
    "# Suppression des colonnes (errors='ignore' permet d'éviter un plantage si une colonne est déjà absente)\n",
    "df = df.drop(columns=cols_unstructured, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes de texte libre et métadonnées ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression de l'Id**\n",
    "\n",
    "L'identifiant unique n'a aucun pouvoir de prédiction sur le comportement financier de l'emprunteur. Le conserver lors de l'entraînement obligerait le modèle de Machine Learning à chercher des corrélations mathématiques là où il n'y a que du bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne 'id'\n",
    "df = df.drop('id', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression de(s) colonne(s) ayant une valeur unique**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La colonne policy_code est un indicateur qui précise si le prêt est conforme aux critères d'acceptation publics de Lending Club.\n",
    "# Valeur = 1 : Le prêt est \"publiquement disponible\". Cela correspond aux données de prêts acceptés que l'on trouve généralement dans les fichiers \"Accepted\".\n",
    "# Valeur = 2 : Le prêt n'est pas publiquement disponible (souvent lié à des produits spécifiques ou des tests de nouveaux modèles de score de crédit par la plateforme).\n",
    "\n",
    "df.groupby('policy_code').size() \\\n",
    "    .reset_index(name='count') \\\n",
    "    .sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absence de variance : 100 % des lignes ont la valeur \"1\".\n",
    "Comme la valeur est la même pour tout le monde, elle n'apporte aucune information pour différencier un bon d'un mauvais payeur.\n",
    "Elle doit donc être supprimée pour ne pas alourdir inutilement le traitement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes constantes ou inutiles identifiées\n",
    "cols_constant = ['policy_code']\n",
    "\n",
    "# Suppression des colonnes (errors='ignore' évite l'erreur si la cellule est relancée)\n",
    "df = df.drop(columns=cols_constant, errors='ignore')\n",
    "\n",
    "print(\"La colonne 'policy_code' (sans variance) a été supprimée avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression de(s) colonne(s) extrêmement corrélées**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection et affichage des 10 premières lignes pour comparaison\n",
    "cols_to_compare = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv']\n",
    "\n",
    "print(\"Aperçu des montants :\")\n",
    "print(df[cols_to_compare].head(10))\n",
    "\n",
    "# Vérification statistique rapide\n",
    "print(\"\\nStatistiques descriptives :\")\n",
    "print(df[cols_to_compare].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loan_amnt (Montant demandé) : C'est le montant total que l'emprunteur a demandé initialement lors de sa demande de prêt. C'est la valeur de base avant toute intervention de la plateforme ou des investisseurs.\n",
    "\n",
    "funded_amnt (Montant financé) : C'est le montant total qui a été engagé pour le prêt.\n",
    "\n",
    "funded_amnt_inv (Montant financé par les investisseurs) : C'est la part du prêt qui a été effectivement financée par des investisseurs individuels (particuliers) sur la plateforme.\n",
    "\n",
    "Ces trois variables sont extrêmement corrélées (Multicolinéarité). \n",
    "\n",
    "Seule loan_amnt est conservée car elle représente l'intention initiale de l'emprunteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes redondantes (fortement corrélées à loan_amnt)\n",
    "cols_redundant = ['funded_amnt', 'funded_amnt_inv']\n",
    "\n",
    "# Suppression des colonnes (errors='ignore' évite l'erreur si la cellule est relancée)\n",
    "df = df.drop(columns=cols_redundant, errors='ignore')\n",
    "\n",
    "print(\"Les colonnes 'funded_amnt' et 'funded_amnt_inv' (redondantes) ont été supprimées avec succès.\")\n",
    "print(f\"Nouvelles dimensions du dataset : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"variable-cible\"></a>\n",
    "### Création de la variable cible (Loan Condition) ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:00.526943Z",
     "iopub.status.busy": "2023-12-13T03:56:00.526423Z",
     "iopub.status.idle": "2023-12-13T03:56:00.706074Z",
     "shell.execute_reply": "2023-12-13T03:56:00.704827Z",
     "shell.execute_reply.started": "2023-12-13T03:56:00.526899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Création de la variable cible (Loan Condition)\n",
    "bad_loan = [\n",
    "    \"Charged Off\", \n",
    "    \"Default\", \n",
    "    \"Does not meet the credit policy. Status:Charged Off\", \n",
    "    \"In Grace Period\",\n",
    "    \"Late (16-30 days)\", \n",
    "    \"Late (31-120 days)\"\n",
    "]\n",
    "\n",
    "df['loan_condition_int'] = df['loan_status'].apply(lambda status: 1 if status in bad_loan else 0).astype(int)\n",
    "df['loan_condition'] = np.where(df['loan_condition_int'] == 0, 'Good Loan', 'Bad Loan')\n",
    "\n",
    "# Vérification rapide\n",
    "df.groupby(['loan_status', 'loan_condition', 'loan_condition_int']).size() \\\n",
    "    .reset_index(name='count') \\\n",
    "    .sort_values(by=['loan_condition', 'count'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transformation-variables\"></a>\n",
    "### Transformation de variables ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de l'ancienneté professionnelle\n",
    "emp_length_mapping = {\n",
    "    '10+ years': 10,\n",
    "    '9 years': 9,\n",
    "    '8 years': 8,\n",
    "    '7 years': 7,\n",
    "    '6 years': 6,\n",
    "    '5 years': 5,\n",
    "    '4 years': 4,\n",
    "    '3 years': 3,\n",
    "    '2 years': 2,\n",
    "    '1 year': 1,\n",
    "    '< 1 year': 0.5,\n",
    "    'n/a': 0\n",
    "}\n",
    "\n",
    "df['emp_length_int'] = df['emp_length'].map(emp_length_mapping)\n",
    "\n",
    "# Vérification rapide\n",
    "df.groupby(['emp_length', 'emp_length_int']).size() \\\n",
    "    .reset_index(name='count') \\\n",
    "    .sort_values(by=['emp_length_int', 'count'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne originale emp_length\n",
    "df.drop(columns=['emp_length'], inplace=True)\n",
    "\n",
    "# Vérification de la présence des colonnes restantes\n",
    "print(f\"La colonne 'emp_length' a été supprimée.\")\n",
    "print(f\"Colonnes actuelles : {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartographie des régions\n",
    "state_to_region = {\n",
    "    'CA': 'West', 'OR': 'West', 'UT': 'West', 'WA': 'West', 'CO': 'West',\n",
    "    'NV': 'West', 'AK': 'West', 'MT': 'West', 'HI': 'West', 'WY': 'West', 'ID': 'West',\n",
    "    'AZ': 'SouthWest', 'TX': 'SouthWest', 'NM': 'SouthWest', 'OK': 'SouthWest',\n",
    "    'GA': 'SouthEast', 'NC': 'SouthEast', 'VA': 'SouthEast', 'FL': 'SouthEast', 'KY': 'SouthEast',\n",
    "    'SC': 'SouthEast', 'LA': 'SouthEast', 'AL': 'SouthEast', 'WV': 'SouthEast', 'DC': 'SouthEast',\n",
    "    'AR': 'SouthEast', 'DE': 'SouthEast', 'MS': 'SouthEast', 'TN': 'SouthEast',\n",
    "    'IL': 'MidWest', 'MO': 'MidWest', 'MN': 'MidWest', 'OH': 'MidWest', 'WI': 'MidWest',\n",
    "    'KS': 'MidWest', 'MI': 'MidWest', 'SD': 'MidWest', 'IA': 'MidWest', 'NE': 'MidWest',\n",
    "    'IN': 'MidWest', 'ND': 'MidWest',\n",
    "    'CT': 'NorthEast', 'NY': 'NorthEast', 'PA': 'NorthEast', 'NJ': 'NorthEast', 'RI': 'NorthEast',\n",
    "    'MA': 'NorthEast', 'MD': 'NorthEast', 'VT': 'NorthEast', 'NH': 'NorthEast', 'ME': 'NorthEast'\n",
    "}\n",
    "\n",
    "df['region'] = df['addr_state'].map(state_to_region)\n",
    "\n",
    "# Vérification rapide\n",
    "df.groupby('region').size() \\\n",
    "    .reset_index(name='count') \\\n",
    "    .sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne addr_state devenue redondante\n",
    "df.drop(columns=['addr_state'], inplace=True)\n",
    "\n",
    "# Confirmation et affichage des colonnes restantes\n",
    "print(f\"La colonne 'addr_state' a été supprimée.\")\n",
    "print(f\"Colonnes géographiques conservées : {[col for col in df.columns if 'region' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the data size to fasten following steps, otherwise the memory will soon run out\n",
    "df_sample = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Automation\n",
    "The following is an EDA(Exploratory Data Analysis) Automation.\n",
    "\n",
    "The code allows for flexible variable selection, enabling more in-depth business insights to be gained.\n",
    "\n",
    "**instruction:**\n",
    "With designed functions, you can quickly plot by entering variables name (the corresponding variable can be selected in the corresponding comment behind the variable). If you run the code in Colab, you can directly select the variable through the drop-down. If not, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:01.134085Z",
     "iopub.status.busy": "2023-12-13T03:56:01.133127Z",
     "iopub.status.idle": "2023-12-13T03:56:02.31199Z",
     "shell.execute_reply": "2023-12-13T03:56:02.310857Z",
     "shell.execute_reply.started": "2023-12-13T03:56:01.134012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EDA_df = df_sample.copy()\n",
    "\n",
    "EDA_cat_columns = EDA_df.select_dtypes(include=['object']).columns.tolist()\n",
    "EDA_num_columns = EDA_df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Filter categorical columns with unique value count less than or equal to the specified threshold.\n",
    "filtered_EDA_cat_columns = [col for col in EDA_cat_columns if EDA_df[col].nunique() <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:02.31489Z",
     "iopub.status.busy": "2023-12-13T03:56:02.314424Z",
     "iopub.status.idle": "2023-12-13T03:56:02.324401Z",
     "shell.execute_reply": "2023-12-13T03:56:02.323127Z",
     "shell.execute_reply.started": "2023-12-13T03:56:02.314847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_variable_distribution(dataframe, variable_name):\n",
    "    if dataframe[variable_name].dtype == 'object':\n",
    "        # Categorical variables, draw a histogram\n",
    "        sns.countplot(x=variable_name, data=dataframe)\n",
    "        plt.xlabel(variable_name)\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Distribution of {variable_name}')\n",
    "    else:\n",
    "        # Numeric variables, plot histograms\n",
    "        sns.histplot(dataframe[variable_name], kde=True)\n",
    "        plt.xlabel(variable_name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Distribution of {variable_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:02.327154Z",
     "iopub.status.busy": "2023-12-13T03:56:02.326659Z",
     "iopub.status.idle": "2023-12-13T03:56:02.763953Z",
     "shell.execute_reply": "2023-12-13T03:56:02.762568Z",
     "shell.execute_reply.started": "2023-12-13T03:56:02.327112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "variable = \"loan_condition\" # @param ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title', 'zip_code', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'fico_range_low', 'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_open_act_il', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med', 'sec_app_mths_since_last_major_derog', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'disbursement_method', 'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term', 'loan_condition_int', 'loan_condition', 'emp_length_int', 'region'] {allow-input: true}\n",
    "plot_variable_distribution(EDA_df, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:02.767139Z",
     "iopub.status.busy": "2023-12-13T03:56:02.76659Z",
     "iopub.status.idle": "2023-12-13T03:56:02.780564Z",
     "shell.execute_reply": "2023-12-13T03:56:02.779473Z",
     "shell.execute_reply.started": "2023-12-13T03:56:02.767091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_2_variable_relationship(dataframe, x_variable, y_variable):\n",
    "    if dataframe[x_variable].dtype != 'object' and dataframe[y_variable].dtype != 'object':\n",
    "        # Two numerical variables, draw a scatter plot\n",
    "        sns.scatterplot(x=x_variable, y=y_variable, data=dataframe)\n",
    "        plt.xlabel(x_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.title(f'Scatter Plot: {x_variable} vs. {y_variable}')\n",
    "    elif dataframe[x_variable].dtype == 'object' and dataframe[y_variable].dtype == 'object':\n",
    "        # Two categorical variables, draw a crosstab\n",
    "        cross_tab = pd.crosstab(index=dataframe[x_variable], columns=dataframe[y_variable], normalize='columns') #Show column summary percentage\n",
    "        sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\")\n",
    "        plt.xlabel(y_variable)\n",
    "        plt.ylabel(x_variable)\n",
    "        plt.title(f'Cross Tabulation: {x_variable} vs. {y_variable}')\n",
    "    elif (dataframe[x_variable].dtype != 'object' and dataframe[y_variable].dtype == 'object') or (dataframe[x_variable].dtype == 'object' and dataframe[y_variable].dtype != 'object'):\n",
    "        # One categorical variable, one numerical variabl\"e, draw a violin plot\n",
    "        if dataframe[x_variable].dtype != 'object':\n",
    "            x_variable, y_variable = y_variable, x_variable  # Swap the order of the variables and make sure Y is a numeric variable\n",
    "        sns.violinplot(x=x_variable, y=y_variable, data=dataframe)\n",
    "        plt.xlabel(x_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.title(f'Violin Plot: {x_variable} vs. {y_variable}')\n",
    "        sns.despine()\n",
    "    else:\n",
    "        print(\"Unsupported combination of variable types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:03.083448Z",
     "iopub.status.busy": "2023-12-13T03:56:03.082615Z",
     "iopub.status.idle": "2023-12-13T03:56:03.894162Z",
     "shell.execute_reply": "2023-12-13T03:56:03.892919Z",
     "shell.execute_reply.started": "2023-12-13T03:56:03.0834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = \"loan_condition\" \n",
    "Y = \"loan_amnt\"\n",
    "\n",
    "plot_2_variable_relationship(EDA_df,X,Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:03.898051Z",
     "iopub.status.busy": "2023-12-13T03:56:03.896751Z",
     "iopub.status.idle": "2023-12-13T03:56:03.916429Z",
     "shell.execute_reply": "2023-12-13T03:56:03.914846Z",
     "shell.execute_reply.started": "2023-12-13T03:56:03.897997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_3_variable_relationship(dataframe, x1_variable, x2_variable, y_variable):\n",
    "    num_types = ['int64', 'float64']\n",
    "\n",
    "    if (dataframe[x1_variable].dtype == 'object' and\n",
    "        dataframe[x2_variable].dtype == 'object' and\n",
    "        dataframe[y_variable].dtype == 'object'):\n",
    "        # Three categorical variables, draw a crosstab\n",
    "        cross_tab = pd.crosstab(index=dataframe[x1_variable], columns=[dataframe[x2_variable], dataframe[y_variable]], normalize='columns')\n",
    "        sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\")\n",
    "        plt.xlabel(f'{x2_variable} - {y_variable}')\n",
    "        plt.ylabel(x1_variable)\n",
    "        plt.title(f'Cross Tabulation: {x1_variable} vs. {x2_variable} vs. {y_variable}')\n",
    "    elif (dataframe[x1_variable].dtype == 'object' and\n",
    "          dataframe[x2_variable].dtype == 'object' and\n",
    "          dataframe[y_variable].dtype in num_types):\n",
    "        # X1 and X2 are categorical, Y is a numerical variable, draw a violin plot or boxplot\n",
    "        # Example: Drawing a Violin Plot\n",
    "        sns.violinplot(x=x1_variable, y=y_variable, hue=x2_variable, data=dataframe)\n",
    "        plt.xlabel(x1_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.title(f'Violin Plot: {x1_variable} vs. {x2_variable} vs. {y_variable}')\n",
    "    elif (dataframe[x1_variable].dtype in num_types and\n",
    "          dataframe[x2_variable].dtype in num_types and\n",
    "          dataframe[y_variable].dtype == 'object'):\n",
    "        # X1 and X2 are numerical variables, Y is a categorical variable, draw a box plot or violin plot\n",
    "        # Example: Drawing a Violin Plot\n",
    "        sns.boxplot(x=x1_variable, y=y_variable, hue=x2_variable, data=dataframe)\n",
    "        plt.xlabel(x1_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.title(f'Box Plot: {x1_variable} vs. {x2_variable} vs. {y_variable}')\n",
    "    elif (dataframe[x1_variable].dtype in num_types and\n",
    "          dataframe[x2_variable].dtype in num_types and\n",
    "          dataframe[y_variable].dtype in num_types):\n",
    "        # Three numerical variables, draw a scatter plot\n",
    "        sns.scatterplot(x=x1_variable, y=x2_variable, hue=y_variable, data=dataframe)\n",
    "        plt.xlabel(x1_variable)\n",
    "        plt.ylabel(x2_variable)\n",
    "        plt.title(f'Scatter Plot: {x1_variable} vs. {x2_variable} vs. {y_variable}')\n",
    "    elif ((dataframe[x1_variable].dtype == 'object' and dataframe[x2_variable].dtype in num_types) or\n",
    "          (dataframe[x1_variable].dtype in num_types and dataframe[x2_variable].dtype == 'object')):\n",
    "        # One of X1 and X2 is a categorical variable and the other is a numerical variable\n",
    "        if dataframe[y_variable].dtype == 'object':\n",
    "            # Y is a categorical variable, draw a boxplot or violin plot\n",
    "            sns.boxplot(x=x1_variable, y=x2_variable, hue=y_variable, data=dataframe)\n",
    "            plt.xlabel(x1_variable)\n",
    "            plt.ylabel(x2_variable)\n",
    "            plt.title(f'Box Plot: {x1_variable} vs. {x2_variable} with Color-Coded {y_variable}')\n",
    "        elif dataframe[y_variable].dtype in num_types:\n",
    "            # Y is a numerical variable, draw a box plot or violin plot\n",
    "            sns.boxplot(x=x1_variable, y=x2_variable, data=dataframe)\n",
    "            plt.xlabel(x1_variable)\n",
    "            plt.ylabel(x2_variable)\n",
    "            plt.title(f'Box Plot: {x1_variable} vs. {x2_variable}')\n",
    "    else:\n",
    "        print(\"Unsupported combination of variable types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:04.167436Z",
     "iopub.status.busy": "2023-12-13T03:56:04.166745Z",
     "iopub.status.idle": "2023-12-13T03:56:06.078366Z",
     "shell.execute_reply": "2023-12-13T03:56:06.07702Z",
     "shell.execute_reply.started": "2023-12-13T03:56:04.167399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X1 = \"grade\"\n",
    "X2 = \"term\" \n",
    "Y = \"loan_amnt\"\n",
    "\n",
    "plot_3_variable_relationship(EDA_df,X1,X2,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Data-Cleaning\"></a>\n",
    "# Data Cleaning\n",
    "\n",
    "This part includes:\n",
    "\n",
    "\n",
    "*   Removing Exclusions\n",
    "*   Missing Value Imputation\n",
    "*   Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Removing-exclusions\"></a>\n",
    "## Removing Exclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete variables with more than 80% missing values**\n",
    "\n",
    "There are a lot of columns which have huge chunk of data missing. These columns are not necessary for our analysis. The following part will drop any columns where 20% or more data is missing, which means only columns whose number of non-null values is at least 80% of the total number of rows in the dataset will be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:15.164511Z",
     "iopub.status.busy": "2023-12-13T03:56:15.163999Z",
     "iopub.status.idle": "2023-12-13T03:56:15.713572Z",
     "shell.execute_reply": "2023-12-13T03:56:15.71265Z",
     "shell.execute_reply.started": "2023-12-13T03:56:15.164471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_missing_value_stats(input_df):\n",
    "    df_null = pd.DataFrame({\n",
    "        'Missing Count': input_df.isnull().sum(),\n",
    "        'Missing Percent': 100 * input_df.isnull().sum() / len(input_df),\n",
    "        'Type': input_df.dtypes\n",
    "    })\n",
    "    missing_values = df_null[df_null['Missing Count'] > 0].sort_values(by='Missing Count', ascending=False) #改\n",
    "    return missing_values\n",
    "\n",
    "def get_value_stats(input_df):\n",
    "    df_null = pd.DataFrame({\n",
    "        '#Count': input_df.notna().sum(),\n",
    "        '%Populated': 100 * input_df.notna().sum() / len(input_df),\n",
    "        '#Unique Values':input_df.nunique(),\n",
    "        'Most Common Value': input_df.mode().iloc[0],\n",
    "        'Type': input_df.dtypes\n",
    "    })\n",
    "\n",
    "    missing_values = df_null[df_null['#Count'] > 0].sort_values(by='#Count', ascending=False)\n",
    "\n",
    "    return missing_values\n",
    "\n",
    "# Finding the the count and percentage of values that are missing.\n",
    "get_missing_value_stats(drop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:16.224981Z",
     "iopub.status.busy": "2023-12-13T03:56:16.224528Z",
     "iopub.status.idle": "2023-12-13T03:56:16.97262Z",
     "shell.execute_reply": "2023-12-13T03:56:16.971467Z",
     "shell.execute_reply.started": "2023-12-13T03:56:16.224944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#drop any columns where over a certain percentage is missing\n",
    "drop_df = drop_df.dropna(axis=1, thresh=int(0.20*len(drop_df)))\n",
    "get_missing_value_stats(drop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Missing-Value-Imputation\"></a>\n",
    "## Missing Value Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><th>index</th><th>Count</th><th>Percent</th><th>Type</th><th>Imputation method</th></tr>\n",
    "  <tr><td>tot_coll_amt</td><td>66689</td><td>24.06</td><td>float64</td><td>0</td></tr>\n",
    "  <tr><td>total_rev_hi_lim</td><td>66689</td><td>24.06</td><td>float64</td><td>0</td></tr>\n",
    "  <tr><td>tot_cur_bal</td><td>66689</td><td>24.06</td><td>float64</td><td>0</td></tr>\n",
    "  <tr><td>emp_length_int</td><td>11101</td><td>4.005</td><td>float64</td><td>median</td></tr>\n",
    "  <tr><td>last_pymnt_d</td><td>921</td><td>0.332</td><td>object</td><td>mode</td></tr>\n",
    "  <tr><td>revol_util</td><td>253</td><td>0.091</td><td>float64</td><td></td></tr>\n",
    "  <tr><td>collections_12_mths_ex_med</td><td>145</td><td>0.052</td><td>float64</td><td></td></tr>\n",
    "  <tr><td>pub_rec</td><td>29</td><td>0.0104</td><td>float64</td><td>median</td></tr>\n",
    "  <tr><td>delinq_2yrs</td><td>29</td><td>0.0104</td><td>float64</td><td>mean</td></tr>\n",
    "  <tr><td>last_credit_pull_d</td><td>24</td><td>0.0086</td><td>object</td><td>mode</td></tr>\n",
    "  <tr><td>annual_income</td><td>4</td><td>0.00144</td><td>float64</td><td>mean</td></tr>\n",
    "  <tr><td>income_category</td><td>4</td><td>0.00144</td><td>object</td><td>mode</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:23.065886Z",
     "iopub.status.busy": "2023-12-13T03:56:23.065444Z",
     "iopub.status.idle": "2023-12-13T03:56:23.382531Z",
     "shell.execute_reply": "2023-12-13T03:56:23.381331Z",
     "shell.execute_reply.started": "2023-12-13T03:56:23.065852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fillna_df = drop_df.copy()\n",
    "\n",
    "# # for object variables - Get the mode of next payment date and last payment date and the last date credit amount was pulled\n",
    "# for column in [\"last_pymnt_d\", \"last_credit_pull_d\"]:  #, 'income_category'\n",
    "#     fillna_df[column] = fillna_df.groupby(\"region\")[column].transform(lambda x: x.fillna(x.mode()))\n",
    "\n",
    "# for numerical variables\n",
    "# Get the mode on the number of accounts in which the client is delinquent\n",
    "fillna_df[\"pub_rec\"] = fillna_df.groupby(\"region\")[\"pub_rec\"].transform(lambda x: x.fillna(x.median()))\n",
    "# Get the mode of the total number of credit lines the borrower has\n",
    "fillna_df[\"total_acc\"] = fillna_df.groupby(\"region\")[\"total_acc\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "fillna_df[\"emp_length_int\"] = fillna_df.groupby(\"region\")[\"emp_length_int\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Get the mean of the annual income depending on the region the client is located.\n",
    "fillna_df[\"annual_inc\"] = fillna_df.groupby(\"region\")[\"annual_inc\"].transform(lambda x: x.fillna(x.mean()))\n",
    "# Mode of credit delinquencies in the past two years.\n",
    "fillna_df[\"delinq_2yrs\"] = fillna_df.groupby(\"region\")[\"delinq_2yrs\"].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:23.487518Z",
     "iopub.status.busy": "2023-12-13T03:56:23.487068Z",
     "iopub.status.idle": "2023-12-13T03:56:23.847555Z",
     "shell.execute_reply": "2023-12-13T03:56:23.846135Z",
     "shell.execute_reply.started": "2023-12-13T03:56:23.487484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # for other, fill in with zero\n",
    "# fillna_df.fillna(0, inplace=True)\n",
    "# fillna_df.isnull().sum().max() # Maximum number of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier et remplir les colonnes numériques avec le chiffre 0\n",
    "cols_num = fillna_df.select_dtypes(include=['number']).columns\n",
    "fillna_df[cols_num] = fillna_df[cols_num].fillna(0)\n",
    "\n",
    "# Identifier et remplir les colonnes de texte avec le texte \"0\" (ou \"Inconnu\", \"Missing\", etc.)\n",
    "cols_str = fillna_df.select_dtypes(include=['object', 'str']).columns\n",
    "fillna_df[cols_str] = fillna_df[cols_str].fillna(\"0\")\n",
    "\n",
    "# Vérifier le nombre maximum de valeurs nulles restantes\n",
    "fillna_df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:23.861498Z",
     "iopub.status.busy": "2023-12-13T03:56:23.861064Z",
     "iopub.status.idle": "2023-12-13T03:56:23.873805Z",
     "shell.execute_reply": "2023-12-13T03:56:23.872848Z",
     "shell.execute_reply.started": "2023-12-13T03:56:23.861466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(fillna_df['loan_condition_int'])\n",
    "# Loan Ratios (Imbalanced classes)\n",
    "fillna_df['loan_condition_int'].value_counts()/len(fillna_df['loan_condition_int']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Removing-Outliers\"></a>\n",
    "## Removing Outliers\n",
    "\n",
    "Custom thresholds were used to remove outliers\n",
    "(3-sigma method did not work well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:25.13715Z",
     "iopub.status.busy": "2023-12-13T03:56:25.136448Z",
     "iopub.status.idle": "2023-12-13T03:56:25.477845Z",
     "shell.execute_reply": "2023-12-13T03:56:25.476616Z",
     "shell.execute_reply.started": "2023-12-13T03:56:25.1371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Custom thresholds\n",
    "RemoveOutlier_df = fillna_df.copy()\n",
    "print(\"Dataset before removing outlier:\",RemoveOutlier_df.shape)\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['annual_inc'] <= 250000]\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['dti'] <= 50]\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['open_acc'] <= 40]\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['total_acc'] <= 80]\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['revol_util'] <= 120]\n",
    "RemoveOutlier_df = RemoveOutlier_df[RemoveOutlier_df['revol_bal'] <= 250000]\n",
    "RemoveOutlier_df.reset_index(drop=True, inplace=True)\n",
    "print(\"Dataset after removing outlier:\",RemoveOutlier_df.shape)\n",
    "\n",
    "RemoveOutlier_df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Correlation-Analysis\"></a>\n",
    "# Correlation Analysis\n",
    "\n",
    "Correlation analysis was performed on the variables to assess their importance and relationship to the target variable y. This provided insights into the most relevant variables for predicting good vs bad loans.\n",
    "\n",
    "For the correlation analysis, categorical variables were label encoded to enable numeric correlation values to be calculated. While this encoding can introduce artificial numerical relationships, it provided a convenient quick view of variable importance.\n",
    "\n",
    "For the actual model building later on, more appropriate encodings like target encoding were used for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:26.09508Z",
     "iopub.status.busy": "2023-12-13T03:56:26.094375Z",
     "iopub.status.idle": "2023-12-13T03:56:26.101535Z",
     "shell.execute_reply": "2023-12-13T03:56:26.100253Z",
     "shell.execute_reply.started": "2023-12-13T03:56:26.095029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target_col = target_variable = \"loan_condition_int\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:26.330121Z",
     "iopub.status.busy": "2023-12-13T03:56:26.329672Z",
     "iopub.status.idle": "2023-12-13T03:56:28.460907Z",
     "shell.execute_reply": "2023-12-13T03:56:28.459657Z",
     "shell.execute_reply.started": "2023-12-13T03:56:26.330086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corr_df = RemoveOutlier_df.copy() \n",
    "\n",
    "# correlation with y\n",
    "correlation_with_loan_condition = corr_df.select_dtypes(include=['int64', 'float64']).corr()[target_variable]\n",
    "sorted_correlation = correlation_with_loan_condition.drop(target_variable).sort_values(ascending=False)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=sorted_correlation.index, y=sorted_correlation.values, orient='v')\n",
    "plt.xlabel('Correlation with{}'.format(target_variable))\n",
    "plt.title('Features Correlation with{}'.format(target_variable))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "print(sorted_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:56:28.463546Z",
     "iopub.status.busy": "2023-12-13T03:56:28.463181Z",
     "iopub.status.idle": "2023-12-13T03:56:29.192411Z",
     "shell.execute_reply": "2023-12-13T03:56:29.191278Z",
     "shell.execute_reply.started": "2023-12-13T03:56:28.463514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select the variables with the highest correlation with the dependent variable and explore the correlation between them\n",
    "top_variables = sorted_correlation.abs().nlargest(10).index.tolist()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "correlation_matrix = RemoveOutlier_df[top_variables].corr()\n",
    "mask = np.tril(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='bwr', vmin=-1, vmax=1, square=True, linewidths=0.5, mask=mask)\n",
    "plt.title('Correlation Heatmap for Numeric Columns of Interest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T03:57:21.228564Z",
     "iopub.status.busy": "2023-12-13T03:57:21.228077Z",
     "iopub.status.idle": "2023-12-13T03:57:54.056423Z",
     "shell.execute_reply": "2023-12-13T03:57:54.055075Z",
     "shell.execute_reply.started": "2023-12-13T03:57:21.228529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Further, explore the specific distribution of the relationship between variables under the action of the dependent variable loan_condition_int\n",
    "# # This runs a bit slowly, so run with caution\n",
    "sample_corr_df = corr_df[top_variables + [target_variable]].sample(n=1000, random_state=42)\n",
    "sns.pairplot(sample_corr_df,hue=target_variable, diag_kind='kde',corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegardes en vue de la partie suivante\n",
    "\n",
    "# Données\n",
    "\n",
    "RemoveOutlier_df.to_parquet(\"DATA/cleaned_data_for_modeling.parquet\")\n",
    "\n",
    "print(\"Données nettoyées sauvegardées pour la phase de feature engineering.\")\n",
    "\n",
    "# Cible\n",
    "\n",
    "with open(\"CONFIG/target_config.txt\", \"w\") as f:\n",
    "    f.write(target_col)\n",
    "\n",
    "print(f\"Configuration sauvegardée : la cible est '{target_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 384604,
     "datasetId": 902,
     "sourceId": 370089,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1120359,
     "datasetId": 608703,
     "sourceId": 1090393,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 487310,
     "datasetId": 217513,
     "sourceId": 471382,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 2386162,
     "datasetId": 1415343,
     "sourceId": 2344503,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 47101,
     "datasetId": 33672,
     "sourceId": 44744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
